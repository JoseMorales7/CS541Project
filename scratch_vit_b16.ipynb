{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision.transforms import transforms\n",
    "import torch\n",
    "import CitiesData\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inference transforms are available at ViT_B_16_Weights.IMAGENET1K_V1.transforms and perform the following preprocessing operations: Accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects. \n",
    "# The images are resized to resize_size=[256] using interpolation=InterpolationMode.BILINEAR, followed by a central crop of crop_size=[224]. \n",
    "# Finally the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
    "\n",
    "#models.ViT_B_16_Weights.IMAGENET1K_V1\n",
    "model_name = \"scratch_vit_b_16_city\"\n",
    "model_image_size = 224\n",
    "vit = models.vit_b_16(weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(torch.nn.Module):\n",
    "    def __init__(self, visionTransformer: models.VisionTransformer):\n",
    "        super(ViT, self).__init__()\n",
    "        \n",
    "        self.reference_vit = visionTransformer\n",
    "        self.reference_vit.heads.head = torch.nn.Linear(768, 768)\n",
    "#         ViTLayers = torch.nn.Sequential(*list(visionTransformer.children())[:-1])\n",
    "\n",
    "#         self.reference_vit.conv_proj.requires_grad = False\n",
    "#         self.reference_vit.encoder.requires_grad = False\n",
    "#         self.reference_vit.heads.requires_grad = False\n",
    "#         self.reference_vit.heads.head.requires_grad = True\n",
    "        \n",
    "#         for param in self.reference_vit.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "        #self.ViT = ViTLayers.to(device)\n",
    "#         self.linear = torch.nn.Linear(1000, 10).to(device)\n",
    "        self.layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 768),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(768, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512 ,256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 10))\n",
    "        self.softmax = torch.nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape and permute the input tensor\n",
    "        x = self._process_input(x)\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Expand the class token to the full batch\n",
    "        batch_class_token = self.reference_vit.class_token.expand(n, -1, -1)\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "\n",
    "        x = self.reference_vit.encoder(x)\n",
    "\n",
    "        # Classifier \"token\" as used by standard language architectures\n",
    "        x = x[:, 0]\n",
    "\n",
    "        x =  self.reference_vit.heads(x)\n",
    "        \n",
    "        #extractedFeature = self.ViT(x)\n",
    "        x = self.layer(x)\n",
    "        softmax = self.softmax(x)\n",
    "\n",
    "        return softmax\n",
    "    \n",
    "    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        n, c, h, w = x.shape\n",
    "        p = self.reference_vit.patch_size\n",
    "        torch._assert(h == self.reference_vit.image_size, f\"Wrong image height! Expected {self.reference_vit.image_size} but got {h}!\")\n",
    "        torch._assert(w == self.reference_vit.image_size, f\"Wrong image width! Expected {self.reference_vit.image_size} but got {w}!\")\n",
    "        n_h = h // p\n",
    "        n_w = w // p\n",
    "        \n",
    "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n",
    "        x =  self.reference_vit.conv_proj(x)\n",
    "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n",
    "        x = x.reshape(n, self.reference_vit.hidden_dim, n_h * n_w)\n",
    "\n",
    "        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n",
    "        # The self attention layer expects inputs in the format (N, S, E)\n",
    "        # where S is the source sequence length, N is the batch size, E is the\n",
    "        # embedding dimension\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    (head): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ") Sequential(\n",
      "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "visionTransformer = ViT(vit).to(device)\n",
    "print(*list(visionTransformer.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(visionTransformer.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.RandomResizedCrop(size=(model_image_size, model_image_size), antialias=True), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainDataLoader, validDataLoader, testDataLoader = CitiesData.getCitiesDataLoader(\"./Data/\", transforms = transform, batchSize=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6101\n",
      "157\n",
      "696\n",
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(len(trainDataLoader))\n",
    "print(len(validDataLoader))\n",
    "print(len(testDataLoader))\n",
    "for i in trainDataLoader:\n",
    "    image, cities, _, _ = i\n",
    "    print(image.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::mul encountered 49 time(s)\n",
      "Unsupported operator aten::add encountered 25 time(s)\n",
      "Unsupported operator aten::div encountered 12 time(s)\n",
      "Unsupported operator aten::unflatten encountered 12 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::softmax encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "reference_vit.encoder.layers.encoder_layer_0.self_attention.out_proj, reference_vit.encoder.layers.encoder_layer_1.self_attention.out_proj, reference_vit.encoder.layers.encoder_layer_10.self_attention.out_proj, reference_vit.encoder.layers.encoder_layer_11.self_attention.out_proj, reference_vit.encoder.layers.encoder_layer_2.self_attention.out_proj, reference_vit.encoder.layers.encoder_layer_3.self_attention.out_proj, reference_vit.encoder.layers.encoder_layer_4.self_attention.out_proj, reference_vit.encoder.layers.encoder_layer_5.self_attention.out_proj, reference_vit.encoder.layers.encoder_layer_6.self_attention.out_proj, reference_vit.encoder.layers.encoder_layer_7.self_attention.out_proj, reference_vit.encoder.layers.encoder_layer_8.self_attention.out_proj, reference_vit.encoder.layers.encoder_layer_9.self_attention.out_proj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539787239424 flops\n"
     ]
    }
   ],
   "source": [
    "from fvcore.nn import FlopCountAnalysis\n",
    "valid_image = 0\n",
    "for i in validDataLoader:\n",
    "    valid_image, cities, _, _ = i\n",
    "    valid_image = valid_image.to(device)\n",
    "    break\n",
    "flops = FlopCountAnalysis(visionTransformer, valid_image)\n",
    "print(str(flops.total()) + \" flops\")\n",
    "# Ignore Reds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_data(vit, dataloader):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        \n",
    "        num_correct = 0.0\n",
    "        num_samples = 0.0\n",
    "        for data in dataloader:\n",
    "            image, city, _, _ = data\n",
    "            city = city.to(device)\n",
    "            image = image.to(device)\n",
    "            outputs = vit(image)\n",
    "            loss = criterion(outputs, city)\n",
    "            total_loss += loss.item()\n",
    "            for i in range(len(city)):\n",
    "\n",
    "                model_vote = 0\n",
    "                answer = 0\n",
    "                for j in range(len(outputs[i])):\n",
    "                    if outputs[i][j] > outputs[i][model_vote]:\n",
    "                        model_vote = j\n",
    "                    if city[i][j] == 1:\n",
    "                        answer = j\n",
    "                \n",
    "                if answer == model_vote:\n",
    "                    num_correct += 1\n",
    "                num_samples += 1\n",
    "                    \n",
    "                \n",
    "                \n",
    "    return total_loss / len(dataloader), num_correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 2.1405, Valid Loss: 2.1258608076324834, Valid ACC: 0.35077906512185375\n",
      "Epoch [1/50], Training Loss: 1.9925, Valid Loss: 2.1017216544893245, Valid ACC: 0.35577307231322414\n",
      "Epoch [1/50], Training Loss: 2.0874, Valid Loss: 2.08800528916082, Valid ACC: 0.3751498202157411\n",
      "Epoch [1/50], Training Loss: 2.0179, Valid Loss: 2.06408062933506, Valid ACC: 0.39532560926887733\n",
      "Epoch [1/50], Training Loss: 1.9344, Valid Loss: 2.0520255173216686, Valid ACC: 0.40391530163803435\n",
      "Epoch [2/50], Training Loss: 1.9208, Valid Loss: 2.0460506046260347, Valid ACC: 0.4125049940071914\n",
      "Epoch [2/50], Training Loss: 1.8995, Valid Loss: 2.0383866845619094, Valid ACC: 0.42349180982820617\n",
      "Epoch [2/50], Training Loss: 2.1284, Valid Loss: 2.0340295860050643, Valid ACC: 0.426887734718338\n",
      "Epoch [2/50], Training Loss: 2.0698, Valid Loss: 2.029736973701199, Valid ACC: 0.4312824610467439\n",
      "Epoch [2/50], Training Loss: 2.0906, Valid Loss: 2.033624822204361, Valid ACC: 0.424690371554135\n",
      "Epoch [3/50], Training Loss: 2.0376, Valid Loss: 2.012297743122967, Valid ACC: 0.446264482620855\n",
      "Epoch [3/50], Training Loss: 1.9971, Valid Loss: 2.0203131767899967, Valid ACC: 0.43887335197762684\n",
      "Epoch [3/50], Training Loss: 2.0021, Valid Loss: 2.0159042102536415, Valid ACC: 0.44206951658010385\n",
      "Epoch [3/50], Training Loss: 1.8799, Valid Loss: 2.007959409175378, Valid ACC: 0.4496604075109868\n",
      "Epoch [3/50], Training Loss: 1.9631, Valid Loss: 2.0133476048789314, Valid ACC: 0.4450659208949261\n",
      "Epoch [4/50], Training Loss: 2.0172, Valid Loss: 2.0047771088860324, Valid ACC: 0.45625249700359566\n",
      "Epoch [4/50], Training Loss: 1.9294, Valid Loss: 2.000190066856699, Valid ACC: 0.45725129844186974\n",
      "Epoch [4/50], Training Loss: 2.0342, Valid Loss: 2.0061903097128684, Valid ACC: 0.45025968837395125\n",
      "Epoch [4/50], Training Loss: 2.0639, Valid Loss: 1.9896668135116489, Valid ACC: 0.4652417099480623\n",
      "Epoch [4/50], Training Loss: 2.0352, Valid Loss: 2.003235310465233, Valid ACC: 0.4540551338393927\n",
      "Epoch [5/50], Training Loss: 2.1411, Valid Loss: 1.9902890550618393, Valid ACC: 0.4628445864962046\n",
      "Epoch [5/50], Training Loss: 1.8182, Valid Loss: 1.9956416188077402, Valid ACC: 0.4614462644826209\n",
      "Epoch [5/50], Training Loss: 2.0150, Valid Loss: 1.984494826142033, Valid ACC: 0.47822612864562525\n",
      "Epoch [5/50], Training Loss: 1.9329, Valid Loss: 1.9732140784911996, Valid ACC: 0.48421893727526966\n",
      "Epoch [5/50], Training Loss: 2.0745, Valid Loss: 1.9755938689684573, Valid ACC: 0.48381941669996004\n",
      "Epoch [6/50], Training Loss: 2.1204, Valid Loss: 1.9683489842481021, Valid ACC: 0.49340791050739113\n",
      "Epoch [6/50], Training Loss: 1.9647, Valid Loss: 1.9631305177940956, Valid ACC: 0.49480623252097483\n",
      "Epoch [6/50], Training Loss: 1.9429, Valid Loss: 1.9636851226448253, Valid ACC: 0.49460647223332\n",
      "Epoch [6/50], Training Loss: 1.8472, Valid Loss: 1.9667056662600837, Valid ACC: 0.49200958849380744\n",
      "Epoch [6/50], Training Loss: 1.8490, Valid Loss: 1.9488492209101613, Valid ACC: 0.5097882540950859\n",
      "Epoch [7/50], Training Loss: 1.9871, Valid Loss: 1.9475233822796594, Valid ACC: 0.5111865761086696\n",
      "Epoch [7/50], Training Loss: 2.0368, Valid Loss: 1.948870851609981, Valid ACC: 0.5113863363963244\n",
      "Epoch [7/50], Training Loss: 2.0218, Valid Loss: 1.9423251520974514, Valid ACC: 0.5137834598481822\n",
      "Epoch [7/50], Training Loss: 1.9659, Valid Loss: 1.9348017732232294, Valid ACC: 0.524770275669197\n",
      "Epoch [7/50], Training Loss: 1.8690, Valid Loss: 1.945110288100339, Valid ACC: 0.5127846584099081\n",
      "Epoch [8/50], Training Loss: 1.9814, Valid Loss: 1.9451250567476353, Valid ACC: 0.5145825009988014\n",
      "Epoch [8/50], Training Loss: 2.0520, Valid Loss: 1.9389174115878816, Valid ACC: 0.520375549340791\n",
      "Epoch [8/50], Training Loss: 1.9667, Valid Loss: 1.952419593116714, Valid ACC: 0.5021973631642029\n",
      "Epoch [8/50], Training Loss: 1.8152, Valid Loss: 1.9332236155793006, Valid ACC: 0.5255693168198162\n",
      "Epoch [8/50], Training Loss: 1.8653, Valid Loss: 1.9338762279637194, Valid ACC: 0.5243707550938873\n",
      "134 sec 1574624 images 1.8162334337830544 loss\u001b\r"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "count = 0\n",
    "valid_loss_array = []\n",
    "valid_acc_array = []\n",
    "train_loss_array = []\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    batch_count = 0\n",
    "    for data in trainDataLoader:\n",
    "        \n",
    "        image, city, _, _ = data\n",
    "        \n",
    "        city = city.to(device)\n",
    "        image = image.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = visionTransformer(image)\n",
    "        loss = criterion(outputs, city)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        end = time.time()\n",
    "        count += 1\n",
    "        print(str(int(end-start)) + \" sec \" + str(count * batch_size) + \" images \" + str(loss.item()) + \" loss\", end='\\x1b\\r')\n",
    "        \n",
    "        batch_count += 1\n",
    "        if batch_count > len(trainDataLoader) / 5.0:\n",
    "            valid_loss, valid_acc = evaluate_on_data(visionTransformer, validDataLoader)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Valid Loss: {valid_loss}, Valid ACC: {valid_acc}')\n",
    "            valid_loss_array.append(valid_loss)\n",
    "            train_loss_array.append(loss.item())\n",
    "            valid_acc_array.append(valid_acc)\n",
    "            batch_count = 0\n",
    "            \n",
    "        \n",
    "    valid_loss, valid_acc = evaluate_on_data(visionTransformer, validDataLoader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Valid Loss: {valid_loss}, Valid ACC: {valid_acc}')\n",
    "    valid_loss_array.append(valid_loss)\n",
    "    train_loss_array.append(loss.item())\n",
    "    valid_acc_array.append(valid_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_name + '_valid.npy', 'wb') as f:\n",
    "    np.save(f, valid_loss_array)\n",
    "    \n",
    "with open(model_name + '_valid_acc.npy', 'wb') as f:\n",
    "    np.save(f, valid_acc_array)\n",
    "    \n",
    "with open(model_name + '_train.npy', 'wb') as f:\n",
    "    np.save(f, train_loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "# this locator puts ticks at regular intervals\n",
    "\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Valid Cross Entropy\")\n",
    "ax[0].plot(range(len(valid_loss_array)), valid_loss_array)\n",
    "\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Valid Accuracy\")\n",
    "ax[1].plot(range(len(valid_acc_array)),valid_acc_array)\n",
    "\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Train Loss\")\n",
    "ax[0].plot(range(len(train_loss_array)), train_loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate_on_data(visionTransformer, testDataLoader)\n",
    "print(str(test_loss) + \"  \" + str(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "8907f5995ab74a6cd5df9da2d2bcd12f57f5b23c9c38358337eeb837f01ad676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
